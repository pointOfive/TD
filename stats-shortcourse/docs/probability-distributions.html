

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Probability Distributions &mdash; stats-shortcourse 1.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
    <link rel="top" title="stats-shortcourse 1.0 documentation" href="index.html"/>
        <link rel="next" title="Bayesian Inference" href="paradigms.html"/>
        <link rel="prev" title="Probability" href="probability.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> stats-shortcourse
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-concepts.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="combinatorics.html">Combinatorics</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability.html">Probability</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Probability Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#properties-of-distributions">Properties of Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#essential-discrete-distributions">Essential Discrete Distributions:</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bernoulli">Bernoulli</a></li>
<li class="toctree-l3"><a class="reference internal" href="#binomial">Binomial</a></li>
<li class="toctree-l3"><a class="reference internal" href="#poisson">Poisson</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#essential-continuous-distributions">Essential Continuous Distributions:</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#uniform">Uniform</a></li>
<li class="toctree-l3"><a class="reference internal" href="#normal-gaussian">Normal/Gaussian</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#less-essential-distributions">Less Essential distributions:</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#geometric">Geometric</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hypergeometric">Hypergeometric</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exponential">Exponential</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#distributional-relationships">Distributional Relationships</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-modeling-considerations">Data Modeling Considerations</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="paradigms.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistics-concepts.html">Statistics Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical-inference.html">Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="regression-classification-metrics.html">Regression, Classification, Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="concluding-remarks.html">Data Science Immersive</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="helpful-math.html">Helpful math</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">Works cited</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">stats-shortcourse</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Probability Distributions</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/probability-distributions.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="probability-distributions">
<h1>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this headline">¶</a></h1>
<p>A probability distribution is a mathematical formalization that describes a
particular type of random process.</p>
<div class="section" id="properties-of-distributions">
<h2>Properties of Distributions<a class="headerlink" href="#properties-of-distributions" title="Permalink to this headline">¶</a></h2>
<p>Probability Distributions are classified into two categories:</p>
<ul class="simple">
<li><strong>discrete</strong> &#8211; producing outcomes that can be mapped to the <em>integers</em> (such as 1, 2, ...)</li>
<li><strong>continuous</strong> &#8211; producing <em>real-valued</em> outcomes (such as 3.14... or 2.71...)</li>
</ul>
<p><strong>Discrete distributions</strong> are specified using
<strong>probability mass functions</strong>
often indicated as <span class="math">\(Pr(X=x)\)</span>
while <strong>continuous distributions</strong>
are specified using <strong>probability density functions</strong>
often indicated as <span class="math">\(f(X=x)\)</span>.</p>
<p><strong>Discrete distributions</strong> specify probabilities of observing outcome <span class="math">\(x\)</span>
from a <strong>discrete</strong> random variable <span class="math">\(X\)</span> directly,
while <strong>continuous distributions</strong> specify
the behavior of realizations <span class="math">\(x\)</span> of a <strong>continuous</strong> random variable <span class="math">\(X\)</span>
in a retaliative rather than absolute manner.
For example,
if <span class="math">\(f(X=x_1) = 2f(X=x_2)\)</span> then in the long-run
<span class="math">\(x_2\)</span> will occur <em>twice as frequently</em> as <span class="math">\(x_1\)</span>.</p>
<p>Regardless of whether or not a
random variable <span class="math">\(X\)</span> is discete or continuous,
if it is distributed according to a distribution named <span class="math">\(XYZ\)</span> with
parameters <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span>, and so on,
then we write</p>
<div class="math">
\[X \sim XYZ(\alpha, \beta, ...)\]</div>
<p>and if
a collection of <span class="math">\(n\)</span> random variables <span class="math">\(X_i, \; i=1, 2, \cdots n\)</span>
are <strong>identically and independently distributed (i.i.d)</strong>
&#8212;i.e., the random variables have <em>the same distribution</em>
and the realization of one <em>does not influence</em> the
realization of another&#8212; then we write</p>
<div class="math">
\[X_i \overset{\small i.i.d.}{\sim} XYZ(\alpha, \beta, ...), \; i=1,2,\cdots n\]</div>
<p>For both continuous and discrete distributions,
there are four properties &#8211; <em>the first through fourth moments</em> &#8211;
that are often used to describe a distribution:</p>
<ul class="simple">
<li><strong>Expectation</strong> (or <strong>Mean</strong>) characterizes the <em>location</em> of a distribution</li>
<li><strong>Variance</strong> or <strong>Standard Deviation</strong> (square root of the <em>variance</em>) characterizes the <em>spread</em> of a distribution</li>
<li><strong>Skewness</strong> characterizes the <em>asymmetry</em> of a distribution</li>
<li><strong>Kurtosis</strong> characterizes the <em>heavy-tailedness</em> of a distribution (i.e., how ofter extreme outlier events occur under the distribution.</li>
</ul>
</div>
<div class="section" id="essential-discrete-distributions">
<h2>Essential Discrete Distributions:<a class="headerlink" href="#essential-discrete-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bernoulli">
<h3>Bernoulli<a class="headerlink" href="#bernoulli" title="Permalink to this headline">¶</a></h3>
<p>A <strong>Bernoulli distribution</strong> is a <em>discrete probability distribution</em>
modeling a one chance attempt that results in either a success
of a failure.  Such an attempt is called a <strong>Bernoulli trial</strong>, with
a success typically recorded as a <span class="math">\(1\)</span> and a failure
typically recorded as a <span class="math">\(0\)</span>.  The most commonly
cited example of a Bernoulli trial is getting a heads on a coin flip,
where the chance of getting a heads is <span class="math">\(50\%\)</span> if the coin is fair
(but another probability otherwise).  The <em>probability mass function</em> for a
<em>Bernoulli</em> random variable <span class="math">\(X\)</span> is defined as</p>
<div class="math">
\[Pr(X=1) = p, \text{ and } Pr(X=0) = 1-p\]</div>
<p>where <span class="math">\(p\)</span> is the <em>parameter</em> of the Bernoulli distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a Bernoulli distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(E[X] = p\)</span></li>
<li><span class="math">\(Var(X) = p(1-p)\)</span></li>
</ul>
<p>(<a class="reference external" href=".//bernoulli-distn.py">Source code</a>, <a class="reference external" href=".//bernoulli-distn.png">png</a>, <a class="reference external" href=".//bernoulli-distn.hires.png">hires.png</a>, <a class="reference external" href=".//bernoulli-distn.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/bernoulli-distn.png" src="_images/bernoulli-distn.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">Let&#8217;s say that I polled all first graders in the state of
colorado and asked the question do you like/dislike your teacher.
The answers are discrete values and the distribution of those
answers could be modeled with a Bernoulli model. What are some other examples?</p>
</div>
<p>Check out this <a class="reference external" href="https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-proportions/v/mean-and-variance-of-bernoulli-distribution-example">khan academy video on the Bernoulli distribution</a> if you need some further intuition about Bernoulli distributions.</p>
</div>
<div class="section" id="binomial">
<h3>Binomial<a class="headerlink" href="#binomial" title="Permalink to this headline">¶</a></h3>
<p>The <strong>binomial distribution</strong> is a <em>discrete probability distribution</em>
that defines the probability of observing exactly <span class="math">\(k\)</span> successes out of
<span class="math">\(n\)</span> identical Bernoulli trials.
The <em>probability mass function</em> for a <em>Binomial</em> random variable <span class="math">\(X\)</span>
is defined as</p>
<div class="math">
\[Pr(X=k) = {n \choose k}p^k(1-p)^{n-k}, \text{ for } k \in \{0, 1,..., n\}\]</div>
<p>where <span class="math">\(p\)</span> and <span class="math">\(n\)</span> are the <em>parameters</em> of the binomial distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a binomial distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(np\)</span></li>
<li><span class="math">\(np(1-p)\)</span></li>
</ul>
<p>(<a class="reference external" href=".//binomial-distn.py">Source code</a>, <a class="reference external" href=".//binomial-distn.png">png</a>, <a class="reference external" href=".//binomial-distn.hires.png">hires.png</a>, <a class="reference external" href=".//binomial-distn.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/binomial-distn.png" src="_images/binomial-distn.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">The number of heads that come from flipping a coin 10 times
is a discrete value that can be modeled
with a binomial distribution. What are some other examples of random variables
that might be modeled with a binomial distribution?</p>
</div>
<p>Check out this <a class="reference external" href="https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-proportions/v/mean-and-variance-of-bernoulli-distribution-example">khan academy video on the binomial distribution</a> if you need some further intuition about binomial distributions.</p>
</div>
<div class="section" id="poisson">
<h3>Poisson<a class="headerlink" href="#poisson" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Poisson distribution</strong> is a <em>discrete probability distribution</em>
that can be used to model the number of times an event
occurs within a given fixed time interval; specifically, it exactly defines a
probability model for the number of arrivals of a sequential process of
<em>exponentially distributed</em> time intervals in that window.
<em>(We will discuss the exponential distribution further below).</em>
<em>(If all of that sequential process process stuff didn&#8217;t make any sense,
don&#8217;t sweat it for now)</em>.  The relationship of the <em>Poisson distribution</em>
to the <em>exponential distribution</em> notwithstanding,
the <em>probability mass function</em> for a <em>Poisson</em> random variable <span class="math">\(X\)</span>
is defined as</p>
<div class="math">
\[Pr(X=k) = \frac{\lambda^k e^{-\lambda}}{k!},\text{ for } k \in \{0,1,2,...\}\]</div>
<p>where <span class="math">\(\lambda\)</span> is the <em>parameter</em> of the Poisson distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a Poisson distribution are, interestingly, the same:</p>
<ul class="simple">
<li><span class="math">\(E[X] = Var(X) = \lambda\)</span></li>
</ul>
<p>(<a class="reference external" href=".//poisson-distn.py">Source code</a>, <a class="reference external" href=".//poisson-distn.png">png</a>, <a class="reference external" href=".//poisson-distn.hires.png">hires.png</a>, <a class="reference external" href=".//poisson-distn.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/poisson-distn.png" src="_images/poisson-distn.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p>The probability that one, two,... uber cars pass in front of
my building in an hour is a discrete value that
could potentially be modeled with a Poisson distribution.
What are some other examples of random variables that could
be modeled as using a Poisson distribution.</p>
<p class="last">Bonus: Can any of these these examples be modeled using a binomial distribution?</p>
</div>
<p>Check out this <a class="reference external" href="https://www.youtube.com/watch?v=3z-M6sbGIZ0">How the Binomial and Poisson Distribution are Related (khan academy) video</a> if you&#8217;re interested in learning a little bit more about the second question in the above exercise. Check out the <a class="reference external" href="https://www.youtube.com/watch?v=Jkr4FSrNEVY">Poisson distribution example (khan academy) video</a> if you need some more intuition about Poisson distributions. And finally, some further <a class="reference external" href="https://www.umass.edu/wsp/resources/poisson">example applications of the Poisson model are discussed here</a>.</p>
</div>
</div>
<div class="section" id="essential-continuous-distributions">
<h2>Essential Continuous Distributions:<a class="headerlink" href="#essential-continuous-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="uniform">
<h3>Uniform<a class="headerlink" href="#uniform" title="Permalink to this headline">¶</a></h3>
<p>The <strong>(continuous) uniform distribution</strong> generates completely
random occurrences over a defined
space.  The <em>probability density function</em>
of the uniform distribution defined over an interval on the real line
is specified as</p>
<div class="math">
\[\begin{split}f(X=x) = \left\{ \begin{array}{c} \frac{1}{b-a}, \text{ for } x \in [a, b]\\ 0, \text { otherwise}\end{array} \right.\end{split}\]</div>
<p>where <span class="math">\(a\)</span> and <span class="math">\(b\)</span> are the <em>parameters</em> of the uniform distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a uniform distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(E[X] = \frac{a+b}{2}\)</span></li>
<li><span class="math">\(Var(X) = \frac{(b-a)^2}{2}\)</span></li>
</ul>
</div>
<div class="section" id="normal-gaussian">
<h3>Normal/Gaussian<a class="headerlink" href="#normal-gaussian" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Gaussian</strong> or <strong>normal distribution</strong>
is a <em>continuous probability distribution</em> whose
<em>probability density function</em> is defined as</p>
<div class="math">
\[f(X=x) = \frac{1}{\sqrt{2\pi\sigma^2}}exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \text{ for } x \in (-\infty, \infty)\]</div>
<p>where <span class="math">\(\mu\)</span> and <span class="math">\(\sigma^2\)</span> are the <em>parameters</em> of the normal
distribution and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a normal distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(E[X] = \mu\)</span></li>
<li><span class="math">\(Var(X) = \sigma^2\)</span></li>
</ul>
<p>The normal or Gaussian distribution
is the distribution most frequently encountered
in statistics.  This is because there is a theorem (<a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">The Central Limit Theorem</a>,
or <strong>CLT</strong>)
that, loosely speaking, says that random variables made up of sums of other random variables
tend to be normally distributed. And since many random variables in our world
are in some regard composite variables in this manner, many of the variables in our world
do appear to be (approximately) normally distributed.  Another reason we come
across the normal distribution so much in statistics is because the CLT
phenomenon can be leveraged as part of hypothesis testing. <em>(We will cover
hypothesis testing tomorrow).</em></p>
<p>(<a class="reference external" href=".//gaussian-distn.py">Source code</a>, <a class="reference external" href=".//gaussian-distn.png">png</a>, <a class="reference external" href=".//gaussian-distn.hires.png">hires.png</a>, <a class="reference external" href=".//gaussian-distn.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/gaussian-distn.png" src="_images/gaussian-distn.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">Test scores, IQs, heights, finishing times from the Boston marathons
have all been empirically shown to be (almost/approximately)
normally distributed. Are you surprised to learn this?</p>
</div>
<p>Check out this <a class="reference external" href="https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/normal-distributions-library/v/introduction-to-the-normal-distribution">khan academy video on the normal distribution</a> if you need some further intuition about normal distributions.</p>
<p><strong>Reparameterization</strong></p>
<p>The way a distribution is <strong>parameterized</strong> is actually an arbitrary
choice. I.e., there many ways way in which the parameters of a distribution
could be be specified.
For example, the inverse of the variance <span class="math">\(\tau = 1/\sigma^{2}\)</span>
is known as the <strong>precision</strong> in a normal distribution context
and we could easily specify the
Gaussian probability density function using the <em>precision</em> <span class="math">\(\tau\)</span> rather
than the <em>variance</em>  <span class="math">\(\sigma^{2}\)</span>.
For that matter, would you say that the Gaussian probability density
function is specified in terms of the variance <span class="math">\(\sigma^{2}\)</span>,
or the standard deviation <span class="math">\(\sigma\)</span>?</p>
</div>
</div>
<div class="section" id="less-essential-distributions">
<h2>Less Essential distributions:<a class="headerlink" href="#less-essential-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="geometric">
<h3>Geometric<a class="headerlink" href="#geometric" title="Permalink to this headline">¶</a></h3>
<p>The <strong>geometric distribution</strong> is a <em>discrete probability distribution</em>
that defines the probability of needing to perform <span class="math">\(k-1\)</span>
identical Bernoulli trials before a success is observed on the <span class="math">\(k^{th}\)</span>
trial.
The <em>probability mass function</em> for a <em>Geometric</em> random variable <span class="math">\(X\)</span>
is defined as</p>
<div class="math">
\[Pr(X=k) = (1-p)^{k-1}p, \text{ for } k \in \{0, 1,...\}\]</div>
<p>where <span class="math">\(p\)</span> is the <em>parameter</em> of the geometric distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using a geometric distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(E[X] = \frac{1}{p}\)</span></li>
<li><span class="math">\(Var(X) = \frac{1-p}{p^2}\)</span></li>
</ul>
<p>Sometimes probabilities of the geometric distribution are given in terms of the
number of failures (<span class="math">\(k-1\)</span>) as opposed to the total tries
(<span class="math">\(k\)</span>, as done above) involved in finally observing a success.</p>
</div>
<div class="section" id="hypergeometric">
<h3>Hypergeometric<a class="headerlink" href="#hypergeometric" title="Permalink to this headline">¶</a></h3>
<p>The <strong>hypergeometric distribution</strong> is a <em>discrete probability distribution</em>
that defines the probability of <span class="math">\(k\)</span> successes from a population of
size <span class="math">\(n\)</span> when sampling <em>without replacement</em>.
To visualize this, think of an urn (&#8220;stats speak&#8221; for &#8220;jar&#8221;) containing two types
of marbles &#8211; say, red and green &#8211;
and define drawing a green marble as a success and drawing a red
marble as a failure. The hypergeometric distribution then defines the
probabilities of the number of marbles that will be green out of
a total of <span class="math">\(n\)</span> marbles sampled from the urn.
The probability of <span class="math">\(k\)</span> successes out of <span class="math">\(n\)</span> attempts in this
context <em>are not</em> binomially distributed because the probability of success
on each subsequent sample changes based on what has been previously drawn
out of the urn.  Stated explicitly,
the <em>probability mass function</em> for a <em>hypergeometric</em> random variable <span class="math">\(X\)</span>
is defined as</p>
<div class="math">
\[Pr(X=k) = \frac{{K \choose k} {{N - K} \choose {n - k}}}{N \choose N}\]</div>
<p>where <span class="math">\(N, K\)</span>, and <span class="math">\(n\)</span> are the parameters of the geometric distribution
specifying the size of the population, the total number of individuals in the
population, and the number of individuals to be sampled for a given
random variable experiment <span class="math">\(X\)</span>, respectively.</p>
<div class="align-center figure">
<a class="reference internal image-reference" href="_images/urns.PNG"><img alt="urns" src="_images/urns.PNG" style="width: 244.3px; height: 114.8px;" /></a>
</div>
<p>The hypergeometric distribution is very interesting because it allows the
mean and variance
of the random variable <span class="math">\(X\)</span> to be independently specified through
the parameters <span class="math">\(N, K\)</span>, and <span class="math">\(n\)</span>, as opposed to the
binomial distribution which only allows one to specify
a deterministic relationship between the mean and variance.
Thus, the clear probabilistic interpretation notwithstanding,
the hypergeometric distribution can also be used in a purely
pragmatic manner to flexibly model count data
that has a different mean and variance combination from those
allowed by the binomial distribution.
Thus, the hypergeometric distribution can essentially be viewed as the
discrete distribution alternative to the normal distribution
in contexts where you want to model <em>counts</em> rather than continuous
values.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">Is there a fundamental difference in the deterministic relationships
between the mean and variance as specified in the binomial distribution
versus the Poisson distribution? Or are the relationships
in some sense a similar type of restriction?</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>PAIRED EXERCISE</strong></p>
<p class="last">Discuss with your neighbor why the formulas for the
<em>geometric</em> and <em>hypergeometric</em> probability mass functions make sense.</p>
</div>
</div>
<div class="section" id="exponential">
<h3>Exponential<a class="headerlink" href="#exponential" title="Permalink to this headline">¶</a></h3>
<p>The <strong>exponential distribution</strong> is a <em>continuous probability distribution</em>
that has proven to be a useful
model (with some deep theoretical justifications) for the
distribution of &#8220;time to arrival&#8221; outcomes. Specifically,
(as we have previously implicitly stated), the exponential distribution
is the distribution of time to arrival outcomes for a so called
<em>Poisson process</em>.  Regardless,
the <em>probability density function</em> for a <em>Geometric</em> random variable <span class="math">\(X\)</span>
is defined as</p>
<div class="math">
\[Pr(X=x) = \lambda e^{-\lambda x}, \text{ for } x \in (0, \infty)\]</div>
<p>where <span class="math">\(\lambda\)</span> is the <em>parameter</em> of the
exponential distribution
and the <em>mean</em> and <em>variance</em> of the random variable
<span class="math">\(X\)</span> modeled using an exponential distribution are, respectively</p>
<ul class="simple">
<li><span class="math">\(E[X] = \frac{1}{\lambda}\)</span></li>
<li><span class="math">\(Var(X) = \frac{1}{\lambda^2}\)</span></li>
</ul>
<p>The exponential distribution has an interesting &#8220;memoryless&#8221; property; namely,
<span class="math">\(Pr(X \geq x+c |X \geq c) = Pr(X \geq x)\)</span>.
What this actually means is that for any
cutpoint <span class="math">\(X \geq c\)</span>, the re-normalized distribution looks exactly the
same as an exponential distribution; only, it&#8217;s been shifted to the right
by <span class="math">\(c\)</span>.</p>
<p>(<a class="reference external" href=".//exponential-distn.py">Source code</a>, <a class="reference external" href=".//exponential-distn.png">png</a>, <a class="reference external" href=".//exponential-distn.hires.png">hires.png</a>, <a class="reference external" href=".//exponential-distn.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/exponential-distn.png" src="_images/exponential-distn.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>PAIRED EXERCISE</strong></p>
<p class="last">The exponential distribution is a special case of the <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a>.
Another special case of the gamma distribution is the <a class="reference external" href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared
distribution</a>.
Have one person explain the chi-squared distribution
and the other explain the gamma distribution.  Then together explain
why the exponential and chi-squared distributions are special cases
of the gamma distribution.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p>Have a look at the documentation for the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html">probability distribution
functionality in SciPy</a>,
which shows how to work with probability distributions using Python.
Specifically, it shows how to use Python to
generate random outcomes from probability distributions &#8211; something
we haven&#8217;t done yet.  For example, here&#8217;s how
to generate random data from from the <em>gamma distribution</em> you
learned about in the last exercise:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">stats</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma_rv</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma_rv</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">50.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma_rv</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
<span class="go">500.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma_rv</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>After you&#8217;ve tried using this code to sample gamma distributed random
variables, try generating some samples from the other
distributions. Play around the specifications of these distributions and
see (a) how the mean and variance parameters of the random variables change
and (b) how these characteristics are reflected in the random samples
drawn from the distributions.</p>
<p class="last">Note: the SciPy implementation of the gamma distribution uses the <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">shape and scale parameterization</a> rather than
the <em>shape and rate parameterization</em>.</p>
</div>
</div>
</div>
<div class="section" id="distributional-relationships">
<h2>Distributional Relationships<a class="headerlink" href="#distributional-relationships" title="Permalink to this headline">¶</a></h2>
<p>We have already come across a couple connections that
exist between different distributions
(i.e., Bernoulli/Biomial, Binomial/Poisson, and Poisson/Exponential).
Actually, there are many
examples of such relationships that exist between probability
distributions.  And, unsurprisingly, there are many, many more distributions
than the ones we covered here.   Here is a diagram
that suggests the scope of things here.</p>
<div class="align-center figure">
<a class="reference internal image-reference" href="_images/relationships.jpg"><img alt="distns" src="_images/relationships.jpg" style="width: 672.0px; height: 354.9px;" /></a>
</div>
</div>
<div class="section" id="data-modeling-considerations">
<h2>Data Modeling Considerations<a class="headerlink" href="#data-modeling-considerations" title="Permalink to this headline">¶</a></h2>
<p>Distributions can be used as models for your data!
As such, there are a few standard considerations to keep in mind in
assessing the appropriateness of a distribution as a potential
data model:</p>
<blockquote>
<div><ul class="simple">
<li>Are my data discrete or continuous?</li>
<li>Are my data symmetric?</li>
<li>What limits are there on possible values for my data?</li>
<li>How likely are extreme values in my data?</li>
<li>Would my data reasonably look like a random sample from this distribution?</li>
</ul>
</div></blockquote>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="paradigms.html" class="btn btn-neutral float-right" title="Bayesian Inference" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="probability.html" class="btn btn-neutral" title="Probability" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Galvanize DSI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>
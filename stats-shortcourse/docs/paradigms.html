

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Inference &mdash; stats-shortcourse 1.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
    <link rel="top" title="stats-shortcourse 1.0 documentation" href="index.html"/>
        <link rel="next" title="Statistics Concepts" href="statistics-concepts.html"/>
        <link rel="prev" title="Probability Distributions" href="probability-distributions.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> stats-shortcourse
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-concepts.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="combinatorics.html">Combinatorics</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability.html">Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-distributions.html">Probability Distributions</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#probability-review">Probability Review</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#conditional-probability">Conditional probability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#statistical-paradigms">Statistical Paradigms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#arguments-for-bayesian-analysis">Arguments for Bayesian Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#are-you-a-bayesian">Are YOU a Bayesian?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#further-study">Further study</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="statistics-concepts.html">Statistics Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical-inference.html">Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="regression-classification-metrics.html">Regression, Classification, Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="concluding-remarks.html">Data Science Immersive</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="helpful-math.html">Helpful math</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">Works cited</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">stats-shortcourse</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Bayesian Inference</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/paradigms.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="bayesian-inference">
<h1>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">¶</a></h1>
<p>Mini-objectives:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Review Probability</li>
<li>Discuss Statistical Philosophies</li>
<li>Repurpose Bayes&#8217; rule for Distributions
(and start callling it Bayes&#8217; Theorem)</li>
<li>Discuss Bayesian Inference</li>
</ol>
</div></blockquote>
<div class="section" id="probability-review">
<h2>Probability Review<a class="headerlink" href="#probability-review" title="Permalink to this headline">¶</a></h2>
<p>Recall that we have learned about <em>three</em> probability estimands</p>
<ul class="simple">
<li>Joint: <span class="math">\(Pr(A \cap B)\)</span></li>
<li>Conditional: <span class="math">\(Pr(A | B)\)</span></li>
<li>Marginal: <span class="math">\(Pr(A)\)</span></li>
</ul>
<p>These concepts <a class="reference external" href="http://sites.nicholas.duke.edu/statsreview/probability/jmc/">are reviewed here</a> and
some related practice problems <a class="reference external" href="http://cecs.wright.edu/~gdong/mining03/tuto1/lesson_1.html">are available here</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p class="last">Specify the <em>conditional</em> probability in terms of
<em>joint</em> and <em>marginal</em> probabilities.</p>
</div>
<div class="section" id="conditional-probability">
<h3>Conditional probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h3>
<p>Recall the exercise from yesterday</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>PREVIOUSLY</strong></p>
<ul class="last simple">
<li>Three types of fair coins are in an urn: HH, HT, and TT</li>
<li>You pull a coin out of the urn, flip it, and it comes up H</li>
<li>Q: what is the probability it comes up H if you flip it a second time?</li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>SOLUTION</strong></p>
<p>The way you solved this problem yesterday was to list out the
sample space and count the outcomes comprising the event of interest.
But of course this problem can also be solved analytically.
To do so we calculate</p>
<ol class="arabic simple">
<li><span class="math">\(Pr(F_1=H \cap F_2=H)\)</span></li>
<li><span class="math">\(Pr(F_1=H)\)</span></li>
<li><span class="math">\(Pr(F_2=H|F_1=H) = \frac{Pr(F_1=H, F_2=H)}{Pr(F_1=H)}\)</span></li>
</ol>
<p>as</p>
<p class="last"><span class="math">\(\begin{eqnarray}Pr(F_1=H \cap F_2=H) &amp;=&amp; \underset{c \in \{HH,HT,TT\}}{\sum}Pr(F_1=H \cap F_2=H | C=c) Pr(C=c)\\&amp;=&amp;1\times\frac{1}{3}+\frac{1}{4}\times\frac{1}{3}+0\times\frac{1}{3} = \frac{5}{12}\\\\\\Pr(F_1=H) &amp;=&amp; \underset{c \in \{HH,HT,TT\}}{\sum}Pr(F_1=H | C=c) Pr(C=c)\\&amp;=&amp;1\times\frac{1}{3}+\frac{1}{2}\times\frac{1}{3}+0\times\frac{1}{3} = \frac{1}{2}\\\\\\\Pr(F_2=H|F_1=H) &amp;=&amp; \frac{5/12}{1/2}=\frac{5}{6}\end{eqnarray}\)</span></p>
</div>
<p>So we first answered this question
<em>counting outcomes in events based on the sample space</em>, and
now we answered it using the <em>law of total probability</em>.
But if you&#8217;re still skeptical about our answer,
perhaps just <em>simulating</em> the experiment will help convince you:</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p>Figure out what the following code does and try it out!</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">coins</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;HH&#39;</span><span class="p">,</span> <span class="s1">&#39;HT&#39;</span><span class="p">,</span> <span class="s1">&#39;TT&#39;</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">coin</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">coins</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">coin</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="s1">&#39;H&#39;</span>

<span class="c1"># df.groupby(&#39;first&#39;).mean()</span>
<span class="c1"># 5./6</span>
</pre></div>
</div>
</div>
<p>Simulation is a great way to confirm your answers to different problem.
It&#8217;s a general purpose tool that you should always remember to keep at
your disposal when you&#8217;re trying to figure out how something works.
Just even the process of creating the simulation can be helpful to nail
down your understanding of a problem.</p>
<p>If you are not familiar with pandas,
here is another way of simulating the conditional probability.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">coins</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;HH&#39;</span><span class="p">,</span> <span class="s1">&#39;HT&#39;</span><span class="p">,</span> <span class="s1">&#39;TT&#39;</span><span class="p">]</span>
<span class="n">coins_selected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">coins</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">first_side_shown</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">c</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">coins_selected</span><span class="p">])</span>
<span class="n">coins_with_heads</span> <span class="o">=</span> <span class="n">coins_selected</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">first_side_shown</span> <span class="o">==</span> <span class="s1">&#39;H&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">second_side_shown</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">c</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">coins_with_heads</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">second_side_shown</span><span class="o">==</span><span class="s1">&#39;H&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">,</span><span class="n">second_side_shown</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">second_side_shown</span><span class="o">==</span><span class="s1">&#39;H&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="o">/</span><span class="n">second_side_shown</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="mi">4096</span><span class="o">/</span><span class="mi">4938</span>
<span class="mf">0.8295</span>
</pre></div>
</div>
<p>More discussion about conditional probabilities can be found <a class="reference external" href="http://sites.nicholas.duke.edu/statsreview/probability/jmc/">here</a>.</p>
</div>
</div>
<div class="section" id="id1">
<h2>Bayesian Inference<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><strong>Bayesian inference</strong> is based on the idea that <em>distributional parameters</em>
<span class="math">\(\theta\)</span> can themselves be viewed as <em>random variables</em> with their
own distributions.  This is distinct from the <strong>Frequentist</strong> perspective which
views parameters as <em>known and fixed constants</em> to be estimated. E.g.,
&#8220;If we measured everyone&#8217;s height instantaneously, at that moment there would
be <em>one true average height</em> in the population.&#8221;  Regardless of one&#8217;s philosophical
perspective, both approaches have value in practice.</p>
<p>The key computational step in the Bayesian framework is deriving the posterior
distribution, which is done using the a forumula we have already seen; namely
Bayes&#8217; theorem:</p>
<div class="math">
\[P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}\]</div>
<p>Bayes&#8217; theorem is comprised of</p>
<blockquote>
<div><ul class="simple">
<li><span class="math">\(P(\theta|X)\)</span> &#8211; the <strong>posterior distribution</strong></li>
<li><span class="math">\(P(X|\theta)\)</span> &#8211; the <strong>likelihood function</strong></li>
<li><span class="math">\(P(\theta)\)</span> &#8211; the <strong>prior distribution</strong></li>
<li><span class="math">\(P(X)\)</span> &#8211; the <strong>marginal likelihood</strong></li>
</ul>
</div></blockquote>
<p>Just as the <em>posterior distribution</em> is the central estimand in Bayesian statistics,
the likelihood function is the central piece of machinery in a Frequentist context.
But as you can see from the formula, the posterior is simply a kind of
&#8220;re-weighting&#8221; of the likelihood function (so Bayesian and frequentist inference
must not be completely at odds &#8211; they agree on at least <em>something</em>).
The re-weighting is accomplished
by striking a balance between the <em>likelihood function</em> and the
<em>prior distribution</em>. The <em>prior</em> distribution represents our belief about the
parameter prior to seeing the data, while the <em>likelihood function</em> tells us
what the data implies about the parameter &#8211; and then these two perspectives
are reconciled.  The <em>marginal likelihood</em> turns out to just be a constant which
ensures that the posterior is a <em>probability mass function</em> or a <em>probability
density function</em> (i.e., sums to one or has area one).  As such, in many
contexts the <em>marginal likelihood</em> simply represents a formality that is not
crucial to the posterior calculation; however, sometimes it is useful (although
it can be difficult to obtain).  Interestingly, the <em>marginal likelihood</em> can be
used for Bayesian model selection, so for some tasks it is an estimand of
primary importance.</p>
</div>
<div class="section" id="statistical-paradigms">
<h2>Statistical Paradigms<a class="headerlink" href="#statistical-paradigms" title="Permalink to this headline">¶</a></h2>
<p><strong>Bayesian inference</strong> works by updating the belief about the parameters
<span class="math">\(\theta\)</span> encoded in the <em>prior distribution</em> with the information
contained in the observed
data <span class="math">\(x\)</span> about the parameters as quantified in the <em>likelihood function</em>.
This updated belief &#8211; called the <em>posterior distribution</em> &#8211;
can serve as the next &#8220;prior&#8221; for the subsequent collection
of additional data, and can itself be updated, and so on.
The updated belief is always encoded as a probability distribution,
so statements of belief about parameters are made using probability
statements. In contrast, <strong>Classical</strong> (or <strong>Frequentist</strong>) <strong>statistics</strong>
instead focusses
on characterizing uncertainty in parameter estimation procedures that
results from random sampling variation. I.e., <em>Frequentist statistics</em>
never makes statements about <em>parameters</em>, but instead makes
statements about probabilities (long-run frequency rates) of
<em>estimation procedures</em>.</p>
<div class="section" id="arguments-for-bayesian-analysis">
<h3>Arguments for Bayesian Analysis<a class="headerlink" href="#arguments-for-bayesian-analysis" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><strong>Ease of Interpretation:</strong>
making probability statements about parameters of interest
is much simpler than trying to perform <em>hypothesis testing</em>
by $interpreting p-values* and
<em>confidence intervals</em> <em>(*to be discussed later).</em></li>
</ul>
<ul class="simple">
<li><strong>No &#8220;Large Sample&#8221; Requirements</strong>:
the accuracy of many Frequentist results rely upon asymptotic distributional
results that require a &#8220;large sample size&#8221; &#8211; the actual quantity
of which often remains unclear &#8211; whereas Bayesian analysis
is a fully coherent probabilistic framework regardless of sample size.</li>
</ul>
<ul class="simple">
<li><strong>Integrated Probabilistic Framework:</strong>
Bayesian analysis provides a hierarchical modeling framework that
definitionally characterizes and propagates all the
modeled uncertainty into parameter estimation.</li>
</ul>
<ul class="simple">
<li><strong>Ability to Utilize Prior Information:</strong>
the Bayesian framework naturally provides a way to
combine information, or <em>learn</em>; <strong>however,
the ability to input (potentially arbitrary) information
into analysis via the prior means objectivity can be sacrificed for
subjectivity.</strong></li>
</ul>
<ul class="simple">
<li><strong>Natural Framework for Regularization:</strong>
the prior distribution of a Bayesian specification can
be used to perform <em>regularization</em>, i.e., stabilize
model fitting procedures so that they are less prone to
overfitting data.</li>
</ul>
<ul>
<li><p class="first"><strong>Complex Data Modeling:</strong>
Bayesian analysis provides &#8211; via computational techniques &#8211;
the ability to develop and use
more complicated modeling specifications than
can be evaluated and use with classical statistical techniques;
however, such approaches can be computationally demanding.</p>
<p><em>In general, Bayesian computation is more expensive than Frequentist
computation as there tends to be a lot of overhead. Also, complex
models are not always preferable: (a) they require practitioners
with more advanced skill sets, (b) they will be more difficult to implement
correctly, and (c) simple solutions can outperform complex solutions
at a fraction of total development and computational costs</em>.</p>
<p><em>Occam&#8217;s razor</em> says that the simplest answer is often correct one.
And <em>Murphy&#8217;s law</em> says that if something can go wrong, it will go wrong.
These are very good considerations to keep in my as you contruct your
data analysis pipelines.</p>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p>What do you appreciate most about the <em>Bayesian philosophy</em>?</p>
<p class="last">What do you appreciate about the <em>Frequentist philosophy</em>?</p>
</div>
</div>
</div>
<div class="section" id="are-you-a-bayesian">
<h2>Are YOU a Bayesian?<a class="headerlink" href="#are-you-a-bayesian" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<blockquote>
<div><strong>CLASS DISCUSSION</strong></div></blockquote>
<ul class="last simple">
<li>You&#8217;re playing poker to win (like your life depends on it!), and the
person you&#8217;re bidding against just tipped his hand a little too low and
you&#8217;ve seen his cards...</li>
<li>You&#8217;re a skilled programmer, but bugs still slip into your code. After a
particularly difficult implementation of an algorithm, you decide to test
your code on a trivial example. It passes. You test the code on a harder
problem. It passes once again. And it passes the next, <em>even more difficult</em>,
test too! You are starting to believe that there may be no bugs in this code...</li>
<li>You&#8217;re a doctor who has some previous experience with the symptoms that are
presenting for the current patient and you&#8217;ve diagnosed this sort of condition
many times before...</li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Are YOU SURE you&#8217;re a Bayesian?</strong></p>
<p>Without looking...</p>
<p class="last">Write Bayes&#8217; theorem and describe the function of the different components
that comprise the theorem, particularly with respect to parameters and evidence.</p>
</div>
</div>
<div class="section" id="further-study">
<h2>Further study<a class="headerlink" href="#further-study" title="Permalink to this headline">¶</a></h2>
<p>If <em>you do</em> actually want to be a Bayesian &#8211; fear not &#8211; you can!
Programming in the Bayesian landscape has become incredibly easy
via the advent of <em>probabilistic programming</em>.
Here are several outstanding resources available
that you can use to start learning more about Bayesian analysis:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.kdnuggets.com/2016/12/datascience-introduction-bayesian-inference.html">Entry level intro posted through kdnuggets</a></li>
<li><a class="reference external" href="https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">Probabilistic Programming and Bayesian Methods for Hackers</a> by <a class="reference external" href="https://github.com/CamDavidsonPilon">Cameron Davidson-Pilon</a></li>
<li><a class="reference external" href="https://github.com/GalvanizeOpenSource/probabilistic-programming-intro">A repository introducing probabilistic programming in Python</a></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="statistics-concepts.html" class="btn btn-neutral float-right" title="Statistics Concepts" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="probability-distributions.html" class="btn btn-neutral" title="Probability Distributions" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Galvanize DSI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>
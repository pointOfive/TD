

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Regression, Classification, Evaluation &mdash; stats-shortcourse 1.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
    <link rel="top" title="stats-shortcourse 1.0 documentation" href="index.html"/>
        <link rel="next" title="Data Science Immersive" href="concluding-remarks.html"/>
        <link rel="prev" title="Statistical Inference" href="statistical-inference.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> stats-shortcourse
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-concepts.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="combinatorics.html">Combinatorics</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability.html">Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-distributions.html">Probability Distributions</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="paradigms.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistics-concepts.html">Statistics Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical-inference.html">Statistical Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Regression, Classification, Evaluation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#predictive-modeling">Predictive Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#types-of-data">Types of Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supervised-learning">Supervised Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-size">Data Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unupervised-learning">Unupervised Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-regression">Linear Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#assumptions">Assumptions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#further-study">Further Study</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="concluding-remarks.html">Data Science Immersive</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="helpful-math.html">Helpful math</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">Works cited</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">stats-shortcourse</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Regression, Classification, Evaluation</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/regression-classification-metrics.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="regression-classification-evaluation">
<h1>Regression, Classification, Evaluation<a class="headerlink" href="#regression-classification-evaluation" title="Permalink to this headline">¶</a></h1>
<p><strong>Objectives</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li>State assumptions of linear regression model</li>
<li>Estimate a linear regression model</li>
<li>Evaluate a linear regression model</li>
</ol>
</div></blockquote>
<div class="section" id="predictive-modeling">
<h2>Predictive Modeling<a class="headerlink" href="#predictive-modeling" title="Permalink to this headline">¶</a></h2>
<p>There are many circumstances in which we are interested in predicting some outcome <span class="math">\(Y\)</span>.
To accomplish this task we set about collecting, selecting, and constructing data
&#8211; a process referred to as <strong>feature engineering</strong> &#8211; that we think would help predict <span class="math">\(Y\)</span>.
Given our selected features <span class="math">\(\textbf{X}\)</span>, the association between the
features <span class="math">\(\textbf{X}\)</span> and and the outcome <span class="math">\(Y\)</span> can be expressed as</p>
<blockquote>
<div><div class="math">
\[Y = f(\mathbf{X}) + \epsilon\]</div>
</div></blockquote>
<p>where <span class="math">\(f\)</span> explicitly describes the precise relationship between
<span class="math">\(\textbf{X}\)</span> and <span class="math">\(Y\)</span>, and <span class="math">\(\epsilon\)</span> is...</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<ul class="last simple">
<li>What is <span class="math">\(\epsilon\)</span>?</li>
<li>How could we model <span class="math">\(\epsilon\)</span>?</li>
<li>How about <span class="math">\(f\)</span>?</li>
<li>Based on what you&#8217;ve seen so far, is <em>Statistics and Probability</em> more suited to describe <span class="math">\(\epsilon\)</span> or <span class="math">\(f\)</span>?</li>
</ul>
</div>
</div>
<div class="section" id="types-of-data">
<h2>Types of Data<a class="headerlink" href="#types-of-data" title="Permalink to this headline">¶</a></h2>
<p>The data <span class="math">\(\textbf{X}\)</span> that we use to predict <span class="math">\(Y\)</span> can come in many varieties, i.e.</p>
<ol class="arabic simple">
<li><strong>Continuous</strong>:<ul>
<li>Price, Quantity, Sales, Tenure</li>
<li>Sometimes it makes sense to map to discrete variables</li>
</ul>
</li>
<li><strong>Categorical</strong>:<ul>
<li>Yes/No, 0/1, Treated/Control, High/Medium/Low</li>
<li>Also called a factor</li>
</ul>
</li>
<li><strong>Missing Values</strong><ul>
<li>May require estimation</li>
</ul>
</li>
<li><strong>&#8220;Non-numeric&#8221;</strong><ul>
<li>Text, Audio, Images, Signals, Graphs</li>
<li>Requires transformation into meaningful quantitative features</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="supervised-learning">
<h2>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Approximating <span class="math">\(f(\textbf{X})\)</span> with some function <span class="math">\(\hat{f}(\textbf{X})\)</span>
in the face of noisy data (as a result of <span class="math">\(\epsilon\)</span>)
is known as <strong>model fitting</strong>. Actually, different disciplines have adopted
different nomenclatures for this process.</p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="34%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Machine Learning</th>
<th class="head">Notation</th>
<th class="head">Other Fields</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>Features</strong></td>
<td><span class="math">\(\textbf{X}_{(n \times p)}\)</span></td>
<td>Covariates, Independent Variables, or Regressors</td>
</tr>
<tr class="row-odd"><td><strong>Targets</strong></td>
<td><span class="math">\(Y_{(n \times 1)}\)</span></td>
<td>Outcome, Dependent or Endogenous Variable</td>
</tr>
<tr class="row-even"><td><strong>Training</strong></td>
<td><span class="math">\(\hat{f}\)</span></td>
<td>Learning, Estimation, or Model Fitting</td>
</tr>
</tbody>
</table>
<p>Depending on the data type of the target,
the <em>supervised learning</em> problem is referred to as either</p>
<blockquote>
<div><ul>
<li><p class="first"><strong>Regression</strong> (when <span class="math">\(Y\)</span> is real-valued)</p>
<blockquote>
<div><p>E.g., if you are predicting price, demand, or size.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<blockquote>
<div><ul>
<li><p class="first"><strong>Classification</strong> (when <span class="math">\(Y\)</span> is categorical)</p>
<blockquote>
<div><p>E.g., if you are prediction fraud or churn</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<div class="section" id="data-size">
<h3>Data Size<a class="headerlink" href="#data-size" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><strong>Associative Studies</strong>: test hypotheses<ul>
<li>Association <em>is</em> causality under carefully controlled conditions</li>
<li>The <strong>power</strong> and accuracy of a test is an asymptotic function of <span class="math">\(N\)</span></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><strong>Predictive Studies</strong>: try to guess well<ul>
<li>Complex models are prone to <strong>overfitting</strong> without sufficient <span class="math">\(N\)</span></li>
<li><strong>Regularization</strong> limits <em>overfitting</em> and <strong>cross-validation</strong> assesses <em>accuracy</em></li>
</ul>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">How would you differentiate <em>Statistics</em> and <em>Machine Learning</em>, if at all?</p>
</div>
</div>
<div class="section" id="unupervised-learning">
<h3>Unupervised Learning<a class="headerlink" href="#unupervised-learning" title="Permalink to this headline">¶</a></h3>
<p>When you&#8217;re not trying to predict a target <span class="math">\(Y\)</span>,
but just seeking to uncover patterns and structures
between the features <span class="math">\(\mathbf{X}\)</span>, the problem is referred to
as <strong>Unsupervised Learning</strong>. The two primary areas of unsupervised
learning are</p>
<blockquote>
<div><ul class="simple">
<li><strong>Clustering</strong>: e.g., hierarchical, k-means</li>
<li><strong>Dimension reduction</strong>: e.g., PCA, SVD, NMF</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Suppose that <span class="math">\(Y_i\)</span> depends on <span class="math">\(X_i\)</span> according to</p>
<div class="math">
\[Y_i = \beta_{0} + \beta_{1} X_i + \epsilon_i, \text{ where } \epsilon_i \overset{\small i.i.d.}{\sim}N\left(0, \sigma^2\right)\]</div>
<p>where <span class="math">\(\beta_{0}\)</span>, <span class="math">\(\beta_{1}\)</span>, and <span class="math">\(\sigma^2\)</span> are the parameters of the model
(intercept, coefficient and variance, respectively).</p>
<p>We can easily simulate some data under an instance of this set of models as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">get_simple_regression_samples</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">b0</span><span class="o">=-</span><span class="mf">0.3</span><span class="p">,</span><span class="n">b1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">error</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">trueX</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">trueT</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="p">(</span><span class="n">b1</span><span class="o">*</span><span class="n">trueX</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">trueX</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">trueT</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">error</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">b0_true</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">b1_true</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">get_simple_regression_samples</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">b0</span><span class="o">=</span><span class="n">b0_true</span><span class="p">,</span><span class="n">b1</span><span class="o">=</span><span class="n">b1_true</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;ko&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">b0_true</span> <span class="o">+</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">b1_true</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;model mean&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//linear-regression.py">Source code</a>, <a class="reference external" href=".//linear-regression_00_00.png">png</a>, <a class="reference external" href=".//linear-regression_00_00.hires.png">hires.png</a>, <a class="reference external" href=".//linear-regression_00_00.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/linear-regression_00_00.png" src="_images/linear-regression_00_00.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>QUESTION</strong></p>
<p class="last">If you added data into the plot above where could you add them that might be a cause for concern?</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p class="last">If you increased to total number of data points generated by this model, how would the density of points in this picture look?</p>
</div>
<p>Now of course in real life you <em>first</em> get your data and <em>then</em> you estimate your model:</p>
<div class="math">
\[\mathbf{y} = \mathbf{X}\mathbf{\hat \beta} + \mathbf{\hat \epsilon}\]</div>
<p>where <span class="math">\(\mathbf{y} = \left[\begin{array}{c}y_1\\y_2\\\vdots\\y_n\end{array}\right]_, \;\;\mathbf{X} = \left[\begin{array}{c}1&amp;x_1\\1&amp;x_2\\\vdots\\1&amp;x_n\end{array}\right]_,  \;\;  \mathbf{\hat \beta} = \left[\begin{array}{c} \hat \beta_0\\ \hat \beta_1 \end{array}\right]\text{ and } \mathbf{\hat \epsilon} = \left[\begin{array}{c}\hat \epsilon_1\\\hat \epsilon_2\\\vdots\\ \hat \epsilon_n\end{array}\right]\)</span></p>
<p>and the predictions from the model are</p>
<div class="math">
\[\mathbf{\hat Y_0} = \mathbf{X_0}\mathbf{\hat \beta}\]</div>
<p>The <strong>residuals</strong> <span class="math">\(\hat \epsilon_i\)</span> are used to estimate the model <strong>mean squared error (MSE)</strong></p>
<div class="math">
\[\displaystyle \frac{n-p-1}{n} \hat \sigma^2 = \sum_{i=1}^n \frac{\epsilon_i^2}{n}\]</div>
<p>where <span class="math">\(p\)</span> is the number of <em>coefficients</em> in the model (here, 1).</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>

<span class="k">def</span> <span class="nf">fit_linear_lstsq</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span><span class="n">ydata</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    y = b0 + b1*x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">xdata</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">matrix</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">xdata</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">basic</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span><span class="n">ydata</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">coefs_lstsq</span> <span class="o">=</span> <span class="n">fit_linear_lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_lstsq</span> <span class="o">=</span> <span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;truth: b0=</span><span class="si">%s</span><span class="s2">,b1=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">b0_true</span><span class="p">,</span><span class="n">b1_true</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;lstsq fit: b0=</span><span class="si">%s</span><span class="s2">,b1=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">3</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p class="last">Try out the above code.  If it&#8217;s making sense to you, try seeing what happends when you change the sample size <span class="math">\(n\)</span>,
or the model intercept <span class="math">\(\beta_0\)</span> and coefficient <span class="math">\(\beta_1\)</span> used to generate the sample.
See if you are able to add the model fit line to the plot of the actual model line itself (from the plot above).</p>
</div>
<div class="section" id="assumptions">
<h3>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h3>
<p>The specification here actually entails many assumptions:</p>
<ol class="arabic">
<li><p class="first"><strong>Fixed and Constant</strong> <span class="math">\(\mathbf{X}\)</span></p>
<p>The <span class="math">\(\mathbf{X}\)</span> are assumed to be measured exactly without error</p>
</li>
</ol>
<ol class="arabic" start="2">
<li><p class="first"><strong>Independent Errors/Outcomes</strong> <span class="math">\(\epsilon/Y\)</span></p>
<p>The final value for any <span class="math">\(Y_i\)</span> (or equivalently, <span class="math">\(\epsilon_i\)</span>) can not be
dependent on any other <span class="math">\(Y_j\)</span> or <span class="math">\(\epsilon_j\)</span>, <span class="math">\(j \not = i\)</span></p>
</li>
</ol>
<ol class="arabic" start="3">
<li><p class="first"><strong>Linear Model Form</strong></p>
<p>The linear relationships as specified by the model are correct.
This is equivalent to having <strong>Unbiased Errors</strong>. I.e., the expected value of the error
<span class="math">\(\epsilon_i\)</span> is 0 for all levels of <span class="math">\(\mathbf{X}\)</span>.</p>
<p>While only linear forms are allowed, the forms are only linear in the model coefficients (not the features).
I.e., any features (e.g., non-linear functions of features like polynomials or spline basis functions)
are permissible.</p>
</li>
</ol>
<ol class="arabic" start="4">
<li><p class="first"><strong>Normal Errors</strong></p>
<p>The errors <span class="math">\(\epsilon_i\)</span> around <span class="math">\(\mathbf{X}\beta\)</span> are normally distributed</p>
</li>
</ol>
<ol class="arabic" start="5">
<li><p class="first"><strong>Homoscedastic Errors</strong></p>
<p>The errors <span class="math">\(\epsilon_i\)</span> have constant variance, <span class="math">\(\sigma^2\)</span>, for all levels of <span class="math">\(\mathbf{X}\)</span>.</p>
</li>
</ol>
<ol class="upperalpha" start="24">
<li><p class="first"><strong>Full Rank of</strong> <span class="math">\(X\)</span></p>
<p>The features are not &#8220;redundant&#8221;; and, being nearly so hurts model performance.</p>
</li>
</ol>
<p>Fortunately, this model can still be effective when some of the assumptions
do not fully hold.  In addition, there are methods available to help address
and correct failures of the assumptions.</p>
<p>Assumptions play a major statistical inference problems (i.e., association studies),
but are less relevant in prediction contexts where it doesn&#8217;t matter how or why it works &#8211;
just whether or not it does. As a result, <em>machine learning</em>
has been able to produce creative and powerful alternatives to the
<em>linear regression model</em> shown above. E.g., k-nearest neighbors, random forests,
gradient boosting, support vector machines, and neural networks.</p>
</div>
</div>
<div class="section" id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<p><strong>Regression</strong></p>
<p>In <em>regression</em> contexts the fit of the model to the data can be assessed using the <em>MSE</em>, from above,
or the <strong>root mean squared error (RMSE)</strong></p>
<div class="math">
\[\displaystyle \sqrt{\sum_{i=1}^n \frac{(y_i-\hat y_i)^2}{n}}\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p class="last">Calculate the RMSE for the data and prediction in the code above.</p>
</div>
<p><strong>Classification</strong></p>
<p>In <em>classification</em> contexts, performance is assessed using a <strong>confusion matrix</strong>:</p>
<table border="1" class="docutils">
<colgroup>
<col width="20%" />
<col width="35%" />
<col width="45%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">&nbsp;</th>
<th class="head">Predicted False <span class="math">\((\hat Y = 0)\)</span></th>
<th class="head">Predicted Ture <span class="math">\((\hat Y = 1)\)</span></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>True <span class="math">\((Y = 0)\)</span></td>
<td>True Negatives <span class="math">\((TN)\)</span></td>
<td>False Negatives <span class="math">\((FN)\)</span></td>
</tr>
<tr class="row-odd"><td>True <span class="math">\((Y = 1)\)</span></td>
<td>False Negatives <span class="math">\((TN)\)</span></td>
<td>True Positives <span class="math">\((TP)\)</span></td>
</tr>
</tbody>
</table>
<p>There are many ways to evaluate the confusion matrix:</p>
<blockquote>
<div><ul class="simple">
<li>Accuracy = <span class="math">\(\frac{TN+TP}{FP+FP+TN+TP}\)</span>: overall proportion correct</li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li>Precision = <span class="math">\(\frac{TP}{TP+FP}\)</span>: proportion called true that are correct</li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li>Recall =  <span class="math">\(\frac{TP}{TP+FN}\)</span>: proportion of true that are called correctly</li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li><span class="math">\(F_1\)</span>-Score = <span class="math">\(\frac{2}{ \frac{1}{recall} + \frac{1}{precision}  }\)</span>: balancing Precision/Recall</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="further-study">
<h2>Further Study<a class="headerlink" href="#further-study" title="Permalink to this headline">¶</a></h2>
<p>A good place to start a review of the content here is:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.youtube.com/watch?v=LvaTokhYnDw&amp;list=PL5-da3qGB5ICcUhueCyu25slvsGp8IDTa">Hastie and Rob Tibshirani (Supervised and Unsupervised learning)</a></li>
<li><a class="reference external" href="https://www.youtube.com/watch?v=WjyuiK5taS8&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">Hastie and Rob Tibshirani (Linear Regression)</a></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="concluding-remarks.html" class="btn btn-neutral float-right" title="Data Science Immersive" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="statistical-inference.html" class="btn btn-neutral" title="Statistical Inference" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Galvanize DSI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>